{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the point of multi-GPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need more than one GPU if model training time consumes a significant fraction of execution pipeline time. \n",
    "Therefore, if you have several GPUs, you can use all of them to train a model. This will speed up the training process of the model.\n",
    "\n",
    "Parameter `device` allows training model on multiple GPU (Сreates a copy of model on each selected GPU).\n",
    "Next, batch data is split across available GPUs and gradients are computed separately and then averaged on one device (usually on the first GPU of the available).\n",
    "\n",
    "Initialization of a large model on a large number of GPUs may take some time (minutes or tens of minutes)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.append('../../..')\n",
    "from batchflow import Pipeline, B, C, V, D, C\n",
    "from batchflow.opensets import Imagenette320\n",
    "from batchflow.models.tf import ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify which GPU(s) to be used. More about it in [CUDA documentation](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3,4,5,6,7\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3,4,5,6,7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset, define a default model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:15<00:15, 15.82s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = Imagenette320(bar=True)\n",
    "\n",
    "model_config = {'inputs/images/shape': B.image_shape,\n",
    "                'inputs/labels/classes': D.num_classes,\n",
    "                'initial_block/inputs': 'images',\n",
    "                'device': C('device'),\n",
    "                'microbatch': C('microbatch')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By default, if one or more GPUs are visible for a pipeline model, a model uses only the first GPU!** Now it's `'GPU:0'`. You can also configure it directly and we will talk about it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'microbatch': None, 'device': None}\n",
    "\n",
    "template = (Pipeline()\n",
    "            .init_variable('loss_history', [])\n",
    "            .init_model('dynamic', ResNet18,'conv_nn', config=model_config)\n",
    "            .resize((320, 320))\n",
    "            .to_array()\n",
    "            .train_model('conv_nn', fetches='loss',\n",
    "                         images=B.images, labels=B.labels,\n",
    "                         save_to=V('loss_history', mode='a'), use_lock=True))\n",
    "\n",
    "pipeline_single = template << dataset.train << config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the next cell execution time is spent on the initialization model. We will compare the initialization time on one GPU and multiple GPUs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 s, sys: 4.21 s, total: 16.4 s\n",
      "Wall time: 16.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.batch_image.ImagesBatch at 0x7f5b15dbf630>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_single.next_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [01:29<00:00,  2.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.pipeline.Pipeline at 0x7f5b153dd0f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_single.run(BATCH_SIZE, shuffle=True, n_epochs=1, bar=True, drop_last=True, prefetch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use `device` and set up 2 GPUs to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({'device': ['GPU:1', 'GPU:2']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter `device` can be string, list of strings, or regular expression.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "'device': 'GPU:0'                     # Used only GPU:0\n",
    "'device': ['GPU:0', 'GPU:1', 'GPU:2'] # Used GPU:0, GPU:1 and GPU:2\n",
    "'device': 'GPU:*'                     # Used all avalible GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Batch size must be divisible by the number of devices!** \\\n",
    "**If `microbatch` is on, microbatch size must be divisible by the number of devices!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on multiple GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_multi = template << dataset.train << config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 s, sys: 3.17 s, total: 31.7 s\n",
      "Wall time: 30.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.batch_image.ImagesBatch at 0x7f595050b6a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_multi.next_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [01:01<00:00,  3.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.pipeline.Pipeline at 0x7f5950496e80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_multi.run(BATCH_SIZE, shuffle=True, n_epochs=1, bar=True, drop_last=True, prefetch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model’s training time is about 1 minute on two GPUs. If we add more GPUs, we get even less training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU and microbathing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Schematic illustration of the formation of batches to each GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Batch_microbatch_GPU.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `microbatch` and `device` at the same time. If we have huge batches, it will be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add microbathing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({'device': ['GPU:3', 'GPU:4'], 'microbatch': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with multiple GPUs and microbatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_multi_micro = template << dataset.train << config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.9 s, sys: 3.4 s, total: 35.3 s\n",
      "Wall time: 32.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.batch_image.ImagesBatch at 0x7f55507819b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_multi_micro.next_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197/197 [02:56<00:00,  1.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.pipeline.Pipeline at 0x7f5550781b00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_multi_micro.run(BATCH_SIZE, shuffle=True, n_epochs=1, bar=True, drop_last=True, prefetch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training finishes without error. It means that we can use the `device` and the `microbatch` together.  \n",
    "\n",
    "Let's look at the training time of the model with one GPU, the model with two GPUs, and the last model. When we added one more GPU to the first model (made the second model), we got to reduce the training process time by 1.5 times! But training time didn't reduce twice due to the appearance of overheads caused by information exchange between multiple GPUs. Furthermore, when we added microbatch to the second model (made the last model), we increased model training time (More about that in [01_microbatch tutorial](./01_microbatch.ipynb)).\n",
    "\n",
    "As stated at the beginning, initialization takes more time on multiple GPUs than on one GPU (see cells with inline magic `%%time`).\n",
    "\n",
    "Using multiple GPUs is a convenient way to speeding up the model training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next tutorial is about [different training procedures](./03_train_steps.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
