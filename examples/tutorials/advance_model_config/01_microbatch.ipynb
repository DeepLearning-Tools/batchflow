{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microbatching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the point of microbathing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, the bigger batch size allows for better gradients estimates, thus helping training models. Unfortunately, there is a hard constraint on device memory: even modern accelerators can't fit more than hundreds of gigabytes, which is sometimes just not enough. This is where `microbatch` comes to rescue: it allows to evenly split batch data into multiple pieces (called microbatches), evaluates gradients on each of them separately, and then apply the averaged value of gradient directly to the model weights.\n",
    "\n",
    "Parameter `microbatch` allows us to leverage simple trade-off between bigger batch size (thus model performance) and model training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('../../..')\n",
    "from batchflow import Pipeline, B, C, V, D\n",
    "from batchflow.opensets import Imagenette320\n",
    "from batchflow.models.tf import ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify which GPU(s) to be used. More about it in [CUDA documentation](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=4\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset, define a default model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "dataset = Imagenette320(bar=True)\n",
    "\n",
    "model_config = {'inputs/images/shape': B.image_shape,\n",
    "                'inputs/labels/classes': D.num_classes,\n",
    "                'initial_block/inputs': 'images',\n",
    "                'microbatch': C('microbatch')}\n",
    "\n",
    "BATCH_SIZE = 4160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model without microbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_wo_microbatch = {'microbatch': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_template = (Pipeline()\n",
    "                  .init_variable('loss_history', [])\n",
    "                  .init_model('dynamic', ResNet18, 'conv_nn', config=model_config)\n",
    "                  .resize((320, 320))\n",
    "                  .to_array()\n",
    "                  .train_model('conv_nn', fetches='loss',\n",
    "                               images=B.images, labels=B.labels,\n",
    "                               save_to=V('loss_history', mode='a')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = train_template << dataset.train << config_wo_microbatch\n",
    "try:\n",
    "    train_pipeline.run(BATCH_SIZE, shuffle=True, n_epochs=1, bar=True, drop_last=True)\n",
    "except tf.errors.ResourceExhaustedError:\n",
    "    print('ResourceExhaustedError')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get **ResourceExhaustedError** because batch didn't fit into GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add microbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add `microbatch` to the model configuration by adding to pipeline config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_with_microbatch = {'microbatch': 64}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, batches will be split into microbatches with size 64 inside the [TFModel.train](https://analysiscenter.github.io/batchflow/api/batchflow.models.tf.base.html#batchflow.models.tf.base.TFModel.train) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Microbatch size must be a divisor of the batch size!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with microbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:06<00:00, 45.82s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<batchflow.pipeline.Pipeline at 0x7f54606817b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_microbatch = train_template << dataset.train << config_with_microbatch\n",
    "pipeline_microbatch.run(BATCH_SIZE, shuffle=True, n_epochs=1, bar=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have `microbatch` in the model configuration, you can control `microbatch` from the `train_model` action directly to dynamically change the splitting strategy. Defining `microbatch` in `train_model` action:\n",
    "```python\n",
    "train_model('conv_nn', fetches='loss',\n",
    "            microbatch=64,\n",
    "            images=B.images, labels=B.labels,\n",
    "            save_to=V('loss_history', mode='a'))\n",
    "```\n",
    "If you defined `microbatch`  in both places (model configuration and train model action), then the value in `train_model` will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **If the data in batch can fit into memory, there is no reason to use `microbatch` due to the inherently slower processing of batches.**\n",
    " \n",
    "Also, `microbatch` affects a model's initialization time. If `microbatch` is on, model initializes slightly longer (due to existence of additional operations in the model e. g. *zero_grads*, *update_grads*, *apply_grads*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train models using `microbatch` and you might want to see next tutorial about [multiple devices](./02_device.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
